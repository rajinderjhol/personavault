# -*- coding: utf-8 -*-
"""Training_RL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O3sAmLecCA6AK8xl2skgl0f8aiuAuZn6

Cell 1: Install Dependencies
"""

# Install necessary libraries
!pip install transformers datasets peft accelerate

"""Cell 2: Upload the Dataset"""

from google.colab import files

# Upload the dataset file
uploaded = files.upload()

# The file will be uploaded to the current working directory
# Replace the dataset_path in the next cells with the uploaded filename

"""Disable wandb Integration"""

import os
os.environ["WANDB_DISABLED"] = "true"

"""Cell 3: Load the Model and Tokenizer"""

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the model and tokenizer
model_name = "Salesforce/codegen-350M-mono"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Assign a pad_token (use eos_token as a substitute if pad_token is not available)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(model_name)

"""Cell 4: Load and Preprocess the Dataset"""

from datasets import load_dataset

# Load the dataset
dataset_path = "your_dataset.jsonl"  # Replace with the uploaded file name
dataset = load_dataset("json", data_files=dataset_path)

# Tokenize the dataset
def preprocess(data):
    return tokenizer(
        data["prompt"],
        text_target=data["response"],
        truncation=True,
        padding="max_length",
        max_length=128  # Adjust as needed
    )

tokenized_dataset = dataset.map(preprocess, batched=True)

# Debug: Check a sample of the tokenized data
print("Sample Tokenized Data:", tokenized_dataset["train"][0])

"""Cell 5: Generate Outputs for Feedback Collection


"""

import json

# Normalize feedback scores to fit the range [0, 4]
def normalize_score(score, max_score=5):
    return min(max(score - 1, 0), max_score - 1)

# Generate outputs for prompts and collect normalized feedback
def collect_feedback(prompt, model, tokenizer, top_k=3):
    """
    Generates multiple outputs for a given prompt and collects normalized feedback from the user.
    """
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True)
    outputs = [
        tokenizer.decode(model.generate(**inputs, max_new_tokens=50)[0], skip_special_tokens=True)
        for _ in range(top_k)
    ]

    # Display generated outputs
    print(f"Prompt: {prompt}")
    print("Generated Outputs:")
    for i, output in enumerate(outputs):
        print(f"{i + 1}: {output}")

    # Collect and normalize feedback scores
    print("\nProvide feedback for each output (1=Bad, 4=Excellent).")
    feedback = [
        normalize_score(int(input(f"Feedback for Output {i + 1}: ")))
        for i in range(top_k)
    ]

    return {"prompt": prompt, "outputs": outputs, "feedback": feedback}

# Collect feedback for the dataset
feedback_data = []
for example in tokenized_dataset["train"]:
    feedback_entry = collect_feedback(example["prompt"], model, tokenizer)
    feedback_data.append(feedback_entry)

# Save the normalized feedback data to a JSON file
with open("feedback_data.json", "w") as f:
    json.dump(feedback_data, f)

print("Feedback collection completed with normalized scores. Feedback data saved to 'feedback_data.json'.")

"""Cell 6: Train a Reward Model"""

from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments

# Step 1: Load Feedback Data and Flatten It
with open("feedback_data.json", "r") as f:
    raw_data = json.loads(f.read())  # Correctly parse the JSON

# Normalize feedback scores to fit the range [0, 4]
def normalize_score(score, max_score=5):
    return min(max(score - 1, 0), max_score - 1)

# Flatten the dataset: each row corresponds to a single output with its feedback
flattened_data = []
for entry in raw_data:
    prompt = entry["prompt"]
    outputs = entry["outputs"]  # List of outputs
    feedback = entry["feedback"]  # List of feedback scores

    # Pair each output with its normalized feedback score
    for output, score in zip(outputs, feedback):
        flattened_data.append({
            "prompt": str(prompt),  # Ensure prompt is a string
            "output": str(output),  # Ensure output is a string
            "label": normalize_score(int(score))  # Normalize feedback score
        })

# Convert the flattened data into a Hugging Face Dataset
reward_dataset = Dataset.from_list(flattened_data)

# Debug: Check the structure of the dataset
print("Sample entry from reward dataset:", reward_dataset[0])

# Step 2: Preprocess the Dataset for Training
reward_model_name = "bert-base-uncased"  # Choose a model suitable for classification
reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_name)

def preprocess_reward_data(batch):
    # Combine the prompt and output for classification
    combined_inputs = [
        str(prompt) + "\n\n" + str(output)
        for prompt, output in zip(batch["prompt"], batch["output"])
    ]
    tokenized = reward_tokenizer(
        combined_inputs,
        truncation=True,
        padding="max_length",
        max_length=128
    )
    tokenized["labels"] = batch["label"]  # Use normalized feedback scores as labels
    return tokenized

# Tokenize the reward dataset
tokenized_reward_dataset = reward_dataset.map(preprocess_reward_data, batched=True)

# Debug: Check a sample from the tokenized dataset
print("Sample Tokenized Entry:", tokenized_reward_dataset[0])

# Step 3: Define and Train the Reward Model
reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_name, num_labels=5)  # Adjust num_labels as needed

reward_training_args = TrainingArguments(
    output_dir="./reward_model",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    save_strategy="epoch"
)

reward_trainer = Trainer(
    model=reward_model,
    args=reward_training_args,
    train_dataset=tokenized_reward_dataset,
    eval_dataset=tokenized_reward_dataset  # Use the same dataset for evaluation if no validation set
)

# Train the reward model
reward_trainer.train()

# Save the reward model and tokenizer
reward_model.save_pretrained("reward_model")
reward_tokenizer.save_pretrained("reward_model")
print("Reward model training completed and saved to 'reward_model'.")

"""Cell 7: Configure LoRA for the Main Model"""

from peft import get_peft_model, LoraConfig, TaskType

# Configure LoRA for the main model
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1
)

model = get_peft_model(model, lora_config)

"""Cell 8: Fine-Tune the Main Model"""

from transformers import TrainingArguments, Trainer

# Training arguments
training_args = TrainingArguments(
    output_dir="./fine_tuned_model",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_steps=10
)

# Fine-tune the model using the tokenized dataset
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["train"]
)

trainer.train()

"""Cell 9: Save the Fine-Tuned Model"""

# Save the fine-tuned model and tokenizer
model.save_pretrained("fine_tuned_rlhf_model")
tokenizer.save_pretrained("fine_tuned_rlhf_model")

"""Cell 10: Test the Fine-Tuned Model"""

# Function to generate code with the fine-tuned model
def generate_code(prompt):
    inputs = tokenizer(prompt, return_tensors="pt", padding=True)
    outputs = model.generate(**inputs, max_new_tokens=100)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Test the model with an example prompt
test_prompt = "Write a JavaScript function that gives the odd numbers between 0-100."
print("Generated Code:\n", generate_code(test_prompt))

"""Cell 11: Compress and Download the Model"""

# Compress the fine-tuned model
!zip -r fine_tuned_rlhf_model.zip fine_tuned_rlhf_model

# Download the compressed model
from google.colab import files
files.download("fine_tuned_rlhf_model.zip")